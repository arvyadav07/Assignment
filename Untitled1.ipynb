{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4Qu65nsM851",
        "outputId": "d5f08208-e4bb-41b0-8acb-1068a5ac54f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1 scraped.\n",
            "Page 2 scraped.\n",
            "Page 3 scraped.\n",
            "Page 4 scraped.\n",
            "Page 5 scraped.\n",
            "Page 6 scraped.\n",
            "Page 7 scraped.\n",
            "Page 8 scraped.\n",
            "Page 9 scraped.\n",
            "Page 10 scraped.\n",
            "Page 11 scraped.\n",
            "Page 12 scraped.\n",
            "Page 13 scraped.\n",
            "Page 14 scraped.\n",
            "Page 15 scraped.\n",
            "Page 16 scraped.\n",
            "Page 17 scraped.\n",
            "Page 18 scraped.\n",
            "Page 19 scraped.\n",
            "Page 20 scraped.\n",
            "No data to write.\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Scrape product details from a given URL\n",
        "def scrape_product_details(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Get the product details\n",
        "    product_url = url\n",
        "    product_name = soup.find('span', {'class': 'a-size-medium a-color-base a-text-normal'}).text.strip()\n",
        "    product_price = soup.find('span', {'class': 'a-offscreen'}).text.strip()\n",
        "    rating = soup.find('span', {'class': 'a-icon-alt'}).text.strip().split(' ')[0]\n",
        "    num_reviews = soup.find('span', {'id': 'acrCustomerReviewText'}).text.strip()\n",
        "\n",
        "    # Return the scraped details as a dictionary\n",
        "    return {\n",
        "        'Product URL': product_url,\n",
        "        'Product Name': product_name,\n",
        "        'Product Price': product_price,\n",
        "        'Rating': rating,\n",
        "        'Number of Reviews': num_reviews\n",
        "    }\n",
        "\n",
        "# Scrape product description from a given URL\n",
        "def scrape_product_description(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Get the product description details\n",
        "    description = soup.find('div', {'id': 'productDescription'}).text.strip()\n",
        "    asin = soup.find('th', text='ASIN').find_next('td').text.strip()\n",
        "    product_description = soup.find('h2', text='Product Description').find_next('div').text.strip()\n",
        "    manufacturer = soup.find('th', text='Manufacturer').find_next('td').text.strip()\n",
        "\n",
        "    # Return the scraped details as a dictionary\n",
        "    return {\n",
        "        'Description': description,\n",
        "        'ASIN': asin,\n",
        "        'Product Description': product_description,\n",
        "        'Manufacturer': manufacturer\n",
        "    }\n",
        "\n",
        "# Scrape multiple product pages\n",
        "def scrape_product_pages(num_pages):\n",
        "    all_products = []\n",
        "\n",
        "    for page in range(1, num_pages + 1):\n",
        "        url = f'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_{page}'\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Find all product links on the page\n",
        "        product_links = soup.find_all('a', {'class': 'a-link-normal a-text-normal'})\n",
        "\n",
        "        for link in product_links:\n",
        "            product_url = 'https://www.amazon.in' + link['href']\n",
        "            product_details = scrape_product_details(product_url)\n",
        "            product_details.update(scrape_product_description(product_url))\n",
        "            all_products.append(product_details)\n",
        "\n",
        "        print(f'Page {page} scraped.')\n",
        "\n",
        "    return all_products\n",
        "\n",
        "\n",
        "\n",
        "# Write scraped data to CSV file\n",
        "def write_to_csv(data, filename):\n",
        "    if not data:\n",
        "        print('No data to write.')\n",
        "        return\n",
        "\n",
        "    keys = data[0].keys()\n",
        "\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=keys)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "\n",
        "    print(f'Data written to {filename} successfully.')\n",
        "\n",
        "\n",
        "# Scrape 20 pages of product listings\n",
        "num_pages = 20\n",
        "products = scrape_product_pages(num_pages)\n",
        "\n",
        "# Write data to CSV file\n",
        "filename = 'amazon_products.csv'\n",
        "write_to_csv(products, filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "koLajfx_NBoA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}